{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bd452e",
   "metadata": {},
   "source": [
    "# EDA Notebook — `Advanced_IoT_Dataset.csv`\n",
    "\n",
    "Generated automated EDA notebook following the provided guidelines.\n",
    "\n",
    "**Contents**:\n",
    "1. Dataset Overview\n",
    "2. Data Quality Checks\n",
    "3. Data Cleaning\n",
    "4. Descriptive Statistics\n",
    "5. Transformation & Encoding\n",
    "6. Outlier Detection & Treatment\n",
    "7. Data Visualization\n",
    "8. Insights & Next Steps\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and load dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "csv_path = r\"/mnt/data/eda_workspace/Advanced_IoT_Dataset.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Loaded:\", csv_path)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7389f3",
   "metadata": {},
   "source": [
    "## 1) Dataset Overview\n",
    "- Show head/tail\n",
    "- Shape, columns, dtypes\n",
    "- Missing and unique counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview: head, tail, shape, columns, dtypes, missing, unique counts\n",
    "display(df.head())\n",
    "display(df.tail())\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"\\nColumns and dtypes:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing values (per column):\")\n",
    "print(df.isnull().sum())\n",
    "print(\"\\nPercentage missing:\")\n",
    "print((df.isnull().mean()*100).round(2))\n",
    "print(\"\\nUnique counts per column:\")\n",
    "print(df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e90b2d",
   "metadata": {},
   "source": [
    "## 2) Data Quality Checks\n",
    "- Duplicates\n",
    "- Erroneous values (simple numeric checks)\n",
    "- Formatting issues (string trimming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003c24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Data quality checks\n",
    "# Duplicate rows\n",
    "dupes = df.duplicated().sum()\n",
    "print(\"Duplicate rows count:\", dupes)\n",
    "\n",
    "# Basic erroneous checks: For numeric columns, check for negative values if they should be non-negative.\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numeric columns:\", num_cols)\n",
    "\n",
    "neg_counts = {}\n",
    "for c in num_cols:\n",
    "    neg = (df[c] < 0).sum()\n",
    "    if neg>0:\n",
    "        neg_counts[c]=int(neg)\n",
    "print(\"Negative values per numeric column (if any):\", neg_counts)\n",
    "\n",
    "# Formatting issues: find leading/trailing spaces in object columns\n",
    "obj_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "strip_issues = {}\n",
    "for c in obj_cols:\n",
    "    # count strings with leading/trailing spaces\n",
    "    s = df[c].astype(str)\n",
    "    mask = s.str.startswith(' ') | s.str.endswith(' ')\n",
    "    cnt = mask.sum()\n",
    "    if cnt>0:\n",
    "        strip_issues[c]=int(cnt)\n",
    "print(\"Columns with leading/trailing spaces (count):\", strip_issues)\n",
    "\n",
    "# Show first 5 rows of columns with issues (if any)\n",
    "if strip_issues:\n",
    "    cols = list(strip_issues.keys())\n",
    "    display(df[cols].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfaa502",
   "metadata": {},
   "source": [
    "## 3) Data Cleaning\n",
    "- Handling missing values (demonstration)\n",
    "- Removing duplicates\n",
    "- Fix simple formatting issues (strip strings)\n",
    "\n",
    "> NOTE: The notebook demonstrates typical steps — adapt imputation choices per domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb4e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Data cleaning (demonstration)\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove exact duplicate rows\n",
    "before = df_clean.shape\n",
    "df_clean = df_clean.drop_duplicates()\n",
    "after = df_clean.shape\n",
    "print(f\"Dropped duplicates: before={before}, after={after}\")\n",
    "\n",
    "# Strip leading/trailing spaces in object columns\n",
    "for c in df_clean.select_dtypes(include=['object']).columns:\n",
    "    df_clean[c] = df_clean[c].astype(str).str.strip()\n",
    "\n",
    "# For numeric columns: fill missing with median (demonstration)\n",
    "for c in df_clean.select_dtypes(include=[np.number]).columns:\n",
    "    if df_clean[c].isnull().sum() > 0:\n",
    "        med = df_clean[c].median()\n",
    "        df_clean[c] = df_clean[c].fillna(med)\n",
    "        print(f\"Filled NA in {c} with median={med}\")\n",
    "\n",
    "# For categorical/object columns: fill missing with mode\n",
    "for c in df_clean.select_dtypes(include=['object']).columns:\n",
    "    if df_clean[c].isnull().sum() > 0:\n",
    "        mode = df_clean[c].mode().iloc[0] if not df_clean[c].mode().empty else ''\n",
    "        df_clean[c] = df_clean[c].fillna(mode)\n",
    "        print(f\"Filled NA in {c} with mode='{mode}'\")\n",
    "\n",
    "print(\"\\nAfter cleaning missing values and formatting:\")\n",
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a4d29",
   "metadata": {},
   "source": [
    "## 4) Descriptive Statistics\n",
    "- Mean, median, mode, min, max, variance, std, skewness, kurtosis\n",
    "- Value counts for categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9744f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Descriptive statistics\n",
    "num = df_clean.select_dtypes(include=[np.number])\n",
    "display(num.describe().T)\n",
    "\n",
    "# Additional stats: variance, skew, kurtosis\n",
    "stats_df = pd.DataFrame({\n",
    "    'variance': num.var(),\n",
    "    'skewness': num.skew(),\n",
    "    'kurtosis': num.kurt()\n",
    "})\n",
    "display(stats_df)\n",
    "\n",
    "# Mode for numeric (if needed) and value_counts for categorical\n",
    "from collections import Counter\n",
    "print(\"\\nCategorical value counts (top 5) for object columns:\")\n",
    "for c in df_clean.select_dtypes(include=['object']).columns:\n",
    "    print(\"\\nColumn:\", c)\n",
    "    print(df_clean[c].value_counts().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698fc348",
   "metadata": {},
   "source": [
    "## 5) Data Transformation & Encoding\n",
    "- Scaling examples: MinMaxScaler and StandardScaler\n",
    "- Example of One-Hot Encoding for a categorical column (if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Transformation & Encoding\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "df_trans = df_clean.copy()\n",
    "\n",
    "numeric_cols = df_trans.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numeric columns:\", numeric_cols)\n",
    "\n",
    "# Apply MinMax scaler example on first up to 3 numeric columns (to keep output short)\n",
    "cols_to_scale = numeric_cols[:3]\n",
    "if cols_to_scale:\n",
    "    scaler = MinMaxScaler()\n",
    "    df_trans[cols_to_scale] = scaler.fit_transform(df_trans[cols_to_scale])\n",
    "    print(\"Applied MinMaxScaler to:\", cols_to_scale)\n",
    "    display(df_trans[cols_to_scale].head())\n",
    "\n",
    "# One-hot encode up to 2 categorical cols (if present)\n",
    "cat_cols = df_trans.select_dtypes(include=['object']).columns.tolist()\n",
    "if cat_cols:\n",
    "    ohe_cols = cat_cols[:2]\n",
    "    df_ohe = pd.get_dummies(df_trans, columns=ohe_cols, drop_first=True)\n",
    "    print(\"One-hot encoded columns:\", ohe_cols)\n",
    "    print(\"New shape after OHE:\", df_ohe.shape)\n",
    "else:\n",
    "    df_ohe = df_trans.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f509a994",
   "metadata": {},
   "source": [
    "## 6) Outlier Detection & Treatment\n",
    "- IQR method and Z-score method demonstration\n",
    "- Boxplot examples (matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e20f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Outlier detection\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "df_out = df_clean.copy()\n",
    "num_cols = df_out.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "outlier_summary = {}\n",
    "# IQR method\n",
    "for c in num_cols:\n",
    "    Q1 = df_out[c].quantile(0.25)\n",
    "    Q3 = df_out[c].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    low = Q1 - 1.5 * IQR\n",
    "    high = Q3 + 1.5 * IQR\n",
    "    mask = (df_out[c] < low) | (df_out[c] > high)\n",
    "    outlier_summary[c] = int(mask.sum())\n",
    "\n",
    "print(\"Outliers detected by IQR (counts):\")\n",
    "print(outlier_summary)\n",
    "\n",
    "# Z-score method: count points with |z| > 3\n",
    "z_summary = {}\n",
    "if num_cols:\n",
    "    z_scores = np.abs(stats.zscore(df_out[num_cols], nan_policy='omit'))\n",
    "    for i, c in enumerate(num_cols):\n",
    "        z_summary[c] = int((z_scores[:, i] > 3).sum())\n",
    "print(\"\\nOutliers detected by Z-score (|z|>3) (counts):\")\n",
    "print(z_summary)\n",
    "\n",
    "# Example boxplot for the first numeric column (if exists)\n",
    "if num_cols:\n",
    "    col = num_cols[0]\n",
    "    plt.figure()\n",
    "    plt.title(f\"Boxplot for {col}\")\n",
    "    plt.boxplot(df_out[col].dropna())\n",
    "    plt.ylabel(col)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f9b34",
   "metadata": {},
   "source": [
    "## 7) Data Visualization\n",
    "- Univariate: histogram, boxplot\n",
    "- Bivariate: scatter and correlation heatmap\n",
    "\n",
    "(Using matplotlib only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Visualizations (matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "num_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Univariate histogram for up to 3 numeric cols\n",
    "for c in num_cols[:3]:\n",
    "    plt.figure()\n",
    "    plt.title(f\"Histogram of {c}\")\n",
    "    plt.hist(df_clean[c].dropna(), bins=30)\n",
    "    plt.xlabel(c)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "# Scatter plot for first two numeric columns (if available)\n",
    "if len(num_cols) >= 2:\n",
    "    x, y = num_cols[0], num_cols[1]\n",
    "    plt.figure()\n",
    "    plt.title(f\"Scatter: {x} vs {y}\")\n",
    "    plt.scatter(df_clean[x], df_clean[y], s=8)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.show()\n",
    "\n",
    "# Correlation matrix heatmap (matplotlib)\n",
    "if num_cols:\n",
    "    corr = df_clean[num_cols].corr()\n",
    "    plt.figure()\n",
    "    plt.title(\"Correlation matrix\")\n",
    "    plt.imshow(corr, interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(num_cols)), num_cols, rotation=90)\n",
    "    plt.yticks(range(len(num_cols)), num_cols)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22436563",
   "metadata": {},
   "source": [
    "## 8) Insights & Next Steps\n",
    "- Summarize brief findings and suggest next steps for modeling\n",
    "\n",
    "**Save notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Quick automated insights (basic)\n",
    "insights = []\n",
    "# Missing values\n",
    "mv = df.isnull().mean()\n",
    "mv = mv[mv>0].sort_values(ascending=False)\n",
    "if not mv.empty:\n",
    "    insights.append(\"Columns with missing values: \" + \", \".join(list(mv.index)))\n",
    "\n",
    "# High cardinality categorical columns\n",
    "for c in df.select_dtypes(include=['object']).columns:\n",
    "    if df[c].nunique() > 50:\n",
    "        insights.append(f\"High cardinality: {c} ({df[c].nunique()} unique)\")\n",
    "\n",
    "# Numeric columns with many outliers (IQR > threshold)\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "for c in num_cols:\n",
    "    Q1 = df[c].quantile(0.25); Q3 = df[c].quantile(0.75); IQR = Q3-Q1\n",
    "    if IQR == 0:\n",
    "        continue\n",
    "    out_pct = ((df[c] < (Q1 - 1.5*IQR)) | (df[c] > (Q3 + 1.5*IQR))).mean()\n",
    "    if out_pct > 0.05:\n",
    "        insights.append(f\"Column {c} has {out_pct:.2%} outliers by IQR (>5%)\")\n",
    "\n",
    "print(\"Automated insights:\")\n",
    "for line in insights[:10]:\n",
    "    print(\"-\", line)\n",
    "\n",
    "# Save a small cleaned sample as csv for reference\n",
    "sample_path = Path(\"/mnt/data/eda_workspace/cleaned_sample.csv\")\n",
    "df_clean.sample(min(200, len(df_clean))).to_csv(sample_path, index=False)\n",
    "print(\"\\nSaved a cleaned sample to:\", sample_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297f73a4",
   "metadata": {},
   "source": [
    "----\n",
    "Notebook generation complete. The notebook file will be saved and is available for download."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
